-- Seed 12 more Eral editorial posts — new categories: gaming, space, energy, education, law, culture
-- Run: npx wrangler d1 execute DB --remote < migrations/0012_seed_more_posts_4.sql

INSERT OR IGNORE INTO editorial_posts
  (id, slug, title, excerpt, content, category, tags, author_id, author_name, published, featured, reading_time, created_at)
VALUES
(
  'ep-023',
  'video-games-are-the-new-literature',
  'Video Games Are the New Literature — Nobody Told the Critics',
  'The art form of our generation is being evaluated by critics who never played one. That needs to change.',
  '<p>There is a peculiar cognitive dissonance at the center of modern cultural criticism. The same publications that celebrate a ten-episode prestige drama about suburban ennui will dismiss a sixty-hour narrative game with the kind of condescension that went out of fashion when people stopped sneering at cinema.</p><p>Video games are the dominant storytelling medium of the twenty-first century. More people play games than watch movies. The industry generates more revenue than film and music combined. And yet the cultural apparatus — the reviews pages, the literary supplements, the awards bodies — continues to treat games as an awkward guest at a dinner party they were not invited to.</p><p>I want to push back on this.</p><p>When *Disco Elysium* arrived in 2019, it was unlike anything the medium had produced: a game built entirely on dialogue, choices, and the interior monologue of a detective dismantling himself in real time. It was literary in the truest sense — not literary as a marketing term, but literary as in it said something true about failure, ideology, and the desperate human need to construct narratives about ourselves.</p><p>Most critics who reviewed it did so with the caveat that they were not "gamers." As if the form required tribal membership to evaluate.</p><p>Compare this to how we talk about cinema. Nobody prefaces a review of *Parasite* with a disclaimer about not being a film person. The medium is taken seriously on its own terms. Its grammar — cuts, close-ups, sound design — is understood as vocabulary, not gimmick.</p><p>Games have grammar too. The choice architecture in *80 Days* tells a story about colonialism through the paths you choose and avoid. The checkpoint-less design of *Celeste* mirrors the exhausting, nonlinear nature of anxiety recovery. These are not accidents. They are authorial decisions embedded in systems, and reading them requires exactly the kind of critical literacy we apply to other art forms.</p><p>The problem is historical. Games grew up alongside the internet, which grew up being sneered at by people who had built careers ignoring it. The first generation of games writers were fans writing for fans — enthusiastic, knowledgeable, but operating outside the apparatus of cultural criticism. By the time the apparatus noticed, it had already decided that what games critics do is not real criticism.</p><p>This is changing, slowly. *The Atlantic* runs serious essays on games. The *Guardian* has expanded its coverage. *Polygon* has published criticism that belongs in any conversation about cultural writing. But the mainstream cultural consensus still treats a BAFTA Games Award as a curiosity rather than a major artistic achievement.</p><p>Meanwhile, the games themselves keep getting better. *Kentucky Route Zero* is one of the finest explorations of American economic despair produced this decade, in any medium. *Return of the Obra Dinn* uses mechanical constraint — you are given only what the evidence shows — to create something close to the feeling of reading a great detective novel. *Elden Ring* is a work of world-building mythology that Tolkien would have appreciated.</p><p>The critics who are not paying attention are missing the conversation of our time.</p><p>There is a generation growing up for whom games are simply the water they swim in — the primary medium through which they encounter narrative, character, consequence, and world. When we fail to develop a serious critical vocabulary for games, we fail that generation. We leave them without the tools to understand what the medium is doing to them and for them.</p><p>I am not arguing that every game is art or that the medium is without its problems. Games have a long history of reducing women to objects, of glorifying violence for its own sake, of prioritizing spectacle over meaning. These are real failures and they deserve serious critical attention.</p><p>But the way to engage those failures is not to treat the medium as inherently trivial. It is to hold it to the standards we apply to everything else we take seriously.</p><p>The critics need to catch up. The games are not waiting for them.</p>',
  'gaming',
  '["gaming", "culture", "art", "criticism", "literature"]',
  'eral-author-001',
  'Eral',
  1, 0, 11,
  datetime('now', '-18 days')
),
(
  'ep-024',
  'the-moon-is-not-a-destination',
  'The Moon Is Not a Destination — It Is a Question About Who Decides',
  'The new space race is less about exploration and more about jurisdiction. The legal vacuum above our heads is being filled right now, and not by democratic process.',
  '<p>The Apollo missions were a government program. Funded by taxes, run by a federal agency, their achievements — and their meaning — belonged to everyone. When Armstrong stepped on the Moon, the message etched on the plaque read: "We came in peace for all mankind." There was no asterisk.</p><p>The space industry being built today is different in kind, not just in scale. It is predominantly private, predominantly American, and predominantly the project of a small number of extraordinarily wealthy individuals who have decided that space is the next frontier for their ambitions. This is not inherently bad. Private capital has always driven exploration. But the legal and philosophical vacuum that surrounds these ventures is beginning to look less like freedom and more like a problem we are choosing not to address until it is too late.</p><p>Consider the Outer Space Treaty of 1967, which remains the foundational document of international space law. It states that no nation may claim sovereignty over celestial bodies. But it was written before private companies existed as meaningful space actors, and it says nothing — nothing at all — about what private entities can do.</p><p>The United States, followed by Luxembourg and several other countries, has passed national legislation asserting that American companies can own and profit from resources extracted from space. This is not illegal under the Outer Space Treaty. It is also not how the treaty was understood when it was negotiated. The legal scholars who wrote it assumed that space activities would remain governmental. The window for that assumption closed sometime in the late 2010s and nobody noticed until afterwards.</p><p>We are now in a period of active norm-setting. The Artemis Accords, promoted by the US and signed by dozens of allied nations, are an attempt to create a framework for commercial space activity. They are also, depending on your perspective, an attempt to lock in Western and American legal principles before China or Russia can negotiate alternatives. China has called them a "space NATO." This is not entirely inaccurate.</p><p>What gets lost in the technical and geopolitical framing is the deeper question: who gets to decide?</p><p>Space is the first genuinely new commons in human history since the deep ocean. We have spent fifty years arguing about how to govern the ocean floor and have not solved it. The governance vacuum above the Kármán Line is filling faster, with higher stakes, and with less democratic input.</p><p>When a company mines an asteroid, the profits will not flow to all of humanity. They will flow to shareholders and, if the company is lucky, to national governments through taxes. The rest of the world gets nothing except the externalities: the debris fields, the light pollution from satellite megaconstellations, and the precedents that make all future space law harder to negotiate.</p><p>I am not a pessimist about space. I think humans in space is genuinely important, both practically and in some harder-to-articulate way about what kind of species we want to be. But I am worried about the gap between the speed of the technology and the speed of the institutions that are supposed to govern it.</p><p>The Moon is not a destination. It is a question about what kind of political economy we want to take with us when we leave this planet. We are answering that question right now, mostly by default, and mostly in favor of the entities that are already there.</p>',
  'space',
  '["space", "law", "politics", "tech", "geopolitics"]',
  'eral-author-001',
  'Eral',
  1, 0, 10,
  datetime('now', '-16 days')
),
(
  'ep-025',
  'the-grid-that-runs-on-wishes',
  'The Grid That Runs on Wishes',
  'The electrical grid is the most complex machine humans have ever built. It is also held together by software written in the 1980s and assumptions that stopped being true a decade ago.',
  '<p>The electrical grid does not store electricity. This is the fact that most people who think they understand the energy transition do not fully reckon with.</p><p>Batteries exist, yes. But at the scale of national electricity infrastructure, storage remains nascent, expensive, and insufficient. The grid is a real-time system: every electron generated must be consumed, almost instantly, or the frequency dips, the voltage drops, and in the worst case, sections of the network begin shedding load in a cascade that can take days to reverse.</p><p>This is not a new problem. Grid operators have managed real-time supply and demand balancing for a hundred years. What is new is the pace at which the composition of the grid is changing — and the institutions governing it are not keeping up.</p><p>Coal and natural gas plants are dispatchable. An operator can call up a gas turbine and tell it to produce more power within minutes. Wind and solar are not dispatchable. They produce electricity when the resource is available, not when demand requires it. This is not a reason not to build them. It is a reason to build the rest of the system — storage, flexible demand, interconnections, fast-responding backup capacity — at the same pace.</p><p>We are not doing that.</p><p>The US electricity grid is a patchwork of regional markets, utility monopolies, federal agencies, and state regulators that evolved over a century of largely separate development. The Eastern Interconnection, the Western Interconnection, and ERCOT in Texas operate almost independently, connected by only a few high-voltage lines that limit how much power can flow between them during a crisis. The rules that govern how generators are compensated were mostly written for a world of large, centralized thermal plants. They function poorly in a world where the grid is increasingly distributed, variable, and bidirectional.</p><p>The software running many of these systems was written in the 1980s and 1990s. Some utilities are still running on SCADA systems so old that the original developers are retired or dead. The cybersecurity implications of this are not hypothetical: the 2021 Oldsmar water treatment hack demonstrated how vulnerable critical infrastructure remains to even relatively unsophisticated attacks.</p><p>Meanwhile, the pace of electrification is accelerating. Electric vehicles are adding load. Heat pumps are adding load. Data centers — fueled in part by AI — are adding load at a rate that is catching grid planners by surprise. The consultancy firm Grid Strategies estimated in 2023 that utility load growth forecasts had shifted from near-zero to potentially doubling by 2035. The wires that carry this power, the substations that transform it, the control systems that manage it — none of this was designed for this scenario.</p><p>I am not saying the energy transition will fail. I am saying the energy transition is a software and institutions problem as much as it is a hardware problem, and we are paying much more attention to the hardware.</p><p>The grid that runs on wishes is the grid that assumes everything will work out because it has to. The grid we actually need is one where we have done the boring, expensive, procedurally complex work of upgrading the institutions as fast as we are upgrading the generation fleet.</p><p>That work is not glamorous. There is no IPO for reforming capacity market rules. But without it, the clean energy future is a system that works most of the time and fails catastrophically when it does not.</p>',
  'energy',
  '["energy", "climate", "infrastructure", "tech", "policy"]',
  'eral-author-001',
  'Eral',
  1, 1, 12,
  datetime('now', '-14 days')
),
(
  'ep-026',
  'what-school-never-taught-you-about-learning',
  'What School Never Taught You About Learning',
  'The science of how the brain actually consolidates knowledge is decades old. It is also almost entirely absent from how we teach. This is not an accident.',
  '<p>There is a finding in cognitive science called the testing effect, and it is one of the most robust results in the field. When you test yourself on material — retrieving information from memory rather than re-reading it — you remember it significantly better. Not a little better. Dramatically, durably better. The research goes back to 1909 and has been replicated hundreds of times across every age group and subject domain.</p><p>Almost no school teaches this.</p><p>Students are told to study by reviewing their notes. They are given homework that confirms understanding rather than demanding retrieval. They take tests at the end of units, which is too late for the testing effect to consolidate the rest of the unit''s material. The pedagogical infrastructure is built around the wrong theory of how learning works.</p><p>This is not the teachers'' fault. Teaching is an extraordinarily difficult job, and most teacher training focuses on classroom management, curriculum delivery, and assessment rather than the cognitive science of memory consolidation. The science lives in psychology departments. The practice lives in schools. The bridge between them is narrower than it should be.</p><p>Let me tell you about spaced repetition. This is the practice of reviewing material at increasing intervals — today, then in two days, then in a week, then in a month — timed to the precise moment before you would forget it. The spacing effect, first described by Hermann Ebbinghaus in 1885, is one of the oldest findings in memory research. It means you can learn the same amount of information in half the time, or twice the information in the same time, simply by distributing practice differently.</p><p>Anki, the free flashcard software that implements spaced repetition algorithms, has hundreds of thousands of users — mostly medical students who discovered that the standard approach to memorizing pharmacology was going to kill them (metaphorically, then potentially literally). They found the research, built the tools, and self-organized a better approach.</p><p>The fact that this happened outside of formal education rather than inside it is a kind of institutional diagnosis.</p><p>Then there is interleaving. Research shows that mixing different problem types in practice — not blocked practice where you do twenty examples of one type, then twenty of another — produces better long-term retention and transfer. It feels harder, which is why students dislike it and why blocked practice remains standard. The difficulty is not a bug. It is the mechanism. Struggling with retrieval strengthens memory in ways that fluent re-reading does not.</p><p>Elaboration: connecting new material to existing knowledge by asking yourself why and how. Concrete examples: anchoring abstract concepts to specific cases. Dual coding: pairing words with visuals. These are not mysterious. They are well-understood features of how human memory works, and they cost nothing to implement.</p><p>So why don''t schools use them?</p><p>The cynical answer: standardized testing optimizes for performance on standardized tests, not for durable learning. Teaching to the test does not require understanding the science of learning; it requires drilling test-adjacent content until it sticks long enough to matter on one particular day.</p><p>The structural answer: curriculum reform is glacially slow. The research-to-practice pipeline in education runs on decades, not years. The institutions that certify teachers, approve textbooks, and set standards are built for stability, not for updating.</p><p>The human answer: learning that works feels harder in the moment. Students resist it, parents resist it, and teachers — who are evaluated on student satisfaction and test scores — have incentives to optimize for what feels like learning rather than what produces it.</p><p>None of these are reasons to give up. But they are reasons to be honest about how far the system is from where the science is.</p>',
  'education',
  '["education", "learning", "science", "psychology", "schools"]',
  'eral-author-001',
  'Eral',
  1, 1, 11,
  datetime('now', '-12 days')
),
(
  'ep-027',
  'the-algorithm-has-no-taste',
  'The Algorithm Has No Taste',
  'Recommendation systems are optimized for engagement, not for quality. The difference matters more than the tech industry has been willing to admit.',
  '<p>A recommendation algorithm does not know what is good. It knows what people clicked on before and predicts what you will click on next. These are not the same thing, and the confusion between them is producing cultural environments of a particular kind of badness.</p><p>Let me be precise about what I mean. When Netflix recommends a show, it is not asking: is this well-made? Does it treat its characters with intelligence? Will you be glad you watched it in six months? It is asking: given the behavior of users similar to you, what has the highest probability of being played for more than seventy seconds, thus registering as a view?</p><p>This is a reasonable thing for a business to optimize for. It is not a reasonable thing to mistake for curation.</p><p>The problem is that the scale of algorithmic recommendation has displaced older systems of cultural discovery — critics, bookshops, word of mouth, the serendipity of a record store browser — without reproducing their functions. The older systems were not perfect. They were gatekept, biased, and often wrong. But they were epistemically different in a way that mattered: they were not optimization processes. They were opinions.</p><p>An opinion can be argued with. It has a position, which means it can be right or wrong, defensible or indefensible. The algorithm has no position. It has a loss function.</p><p>I think about this when I look at what recommendation systems have done to music. Spotify''s Discover Weekly is genuinely useful — it surfaces music I would not have found on my own, and some of it is excellent. But the long-term effect on the music ecosystem is to advantage music optimized for streaming: shorter songs, hook-in-the-first-ten-seconds structures, less dynamic range, more sonic consistency. These are not aesthetic choices made by artists. They are adaptations to an engagement function.</p><p>Books are heading the same way. Amazon''s recommendation system advantages certain genres and certain cover aesthetics and certain opening-chapter pacing conventions. Not because these are better books, but because they are books that perform well against the metric of: did the person who downloaded the sample go on to purchase the full book?</p><p>The most insidious version of this is what happens to news and nonfiction. Recommendation systems in news optimize for what people engage with emotionally, which is not the same as what is true, important, or worth your time. We have been running this experiment for fifteen years and the results are in: people are more anxious, more angry, more certain of things that are wrong, and less trusting of institutions than they were before.</p><p>I am not saying technology caused this. I am saying technology accelerated it, and built business models on top of it.</p><p>The alternative is not a return to a gatekept past. It is to develop something we do not yet have very well: systems and practices for algorithmic curation that optimize for quality rather than engagement, and that are transparent enough to be argued with.</p><p>Some things exist. Letterboxd is a recommendation system built around explicit ratings from real humans who have opinions. Goodreads, despite its problems, has the seed of something similar. Substack''s discovery layer is built around editorial judgment made by editors you choose to trust.</p><p>These are small and imperfect. But they are epistemically different from engagement optimization. They have taste, or something close enough to argue with.</p><p>That is what we need more of.</p>',
  'tech',
  '["tech", "algorithms", "culture", "media", "AI"]',
  'eral-author-001',
  'Eral',
  1, 0, 10,
  datetime('now', '-10 days')
),
(
  'ep-028',
  'food-is-not-content',
  'Food Is Not Content',
  'The rise of food media has made everyone a critic and nobody a cook. There is something being lost in the attention economy''s approach to eating.',
  '<p>Something happened to food somewhere around 2012, when Instagram made the overhead shot of a flat white as common as the flat white itself. The thing that happened is this: food became content.</p><p>This is not entirely bad. Democratizing food media has meant that regional cuisines, immigrant traditions, and working-class food cultures have gotten attention and respect they never would have received from the old apparatus of white-tablecloth restaurant criticism. The fermentation revival, the global hot sauce moment, the reclamation of so-called peasant foods — none of this would have happened at the same pace without social media.</p><p>But something has also been lost, and I want to try to name it.</p><p>When food becomes content, it gets optimized for the platform. This means it optimizes for visual impact over flavor, for novelty over depth, for the first bite over the fifth. The burgers get taller until they are structurally impossible to eat. The desserts get more extreme until they are theater first and food second. The recipes get shorter and faster until the slow cooking processes that produce depth of flavor are designed out.</p><p>I am not being a purist here. I understand that format shapes content and content shapes culture and this is just how media works. But the specific optimization of food media toward visual spectacle has downstream effects on actual cooking culture that I think are worth examining.</p><p>Consider the cooking show. The old cooking show — Julia Child, Keith Floyd, the Galloping Gourmet — was fundamentally instructional. It was built around the premise that the viewer might cook the thing. The new cooking show, from the compressed drama of MasterChef to the ten-second TikTok technique, is built around the premise that the viewer will watch the thing. The pleasure is vicarious. The skill transfer is incidental.</p><p>This is not the fault of individual creators or viewers. It is what the attention economy does: it extracts the pleasurable surface of an activity and delivers that surface as a substitute for the activity itself. You can watch forty episodes of The Great British Bake Off without learning to make pastry. You can follow four hundred food accounts without learning to cook.</p><p>I cook badly and slowly, with plenty of failures. But I cook. And I notice that the people in my life who cook — who have the patience to brown onions slowly, to taste and adjust, to understand why a sauce breaks and how to fix it — have a relationship with food that no amount of content consumption produces.</p><p>The knowledge is in the hands, not the eyes. The pleasure is in the process, not the scroll.</p><p>Food is a practice. Content is a commodity. These are not the same thing, and the conflation is producing a generation of excellent food photographers who have never made their own broth.</p>',
  'culture',
  '["food", "culture", "media", "tech", "society"]',
  'eral-author-001',
  'Eral',
  1, 0, 8,
  datetime('now', '-9 days')
),
(
  'ep-029',
  'why-copyright-is-breaking-the-internet',
  'Why Copyright Is Breaking the Internet',
  'The legal framework designed for printed books is governing the internet, and it is producing outcomes that serve nobody except the lawyers.',
  '<p>The internet was built on copying. This is not a metaphor. When you load a webpage, your browser downloads a copy of the HTML, the images, the scripts. When a search engine indexes the web, it makes copies of everything it crawls. When you send an email with an image, that image is copied to the sender''s server, the recipient''s server, and probably several backup servers along the way.</p><p>Copyright law was not designed for this. It was designed for a world of physical objects, where making a copy required significant physical effort — a printing press, a photocopier — and where copies were therefore scarce and valuable. The legal frameworks we have imported into the digital world were built for a different reality, and they are producing outcomes that range from absurd to actively harmful.</p><p>Consider the DMCA takedown system, the mechanism by which copyright owners can demand that platforms remove infringing content. In theory, this is a balanced system: copyright holders get a tool to protect their work, platforms get a safe harbor from liability, and disputed claims are resolved by a counter-notification process.</p><p>In practice, the system is weaponized at scale. Large rights holders use automated systems to send millions of takedowns, many of which are erroneous. A musician covering her own song can receive a takedown from an algorithm that matches her recording against the original she wrote. A documentary filmmaker can lose years of footage to a music licensing dispute over eight seconds of background television. Political opponents are using spurious copyright claims to silence criticism. The counter-notification process requires legal knowledge and courage that most individuals do not have.</p><p>The underlying problem is that copyright terms have expanded far beyond any plausible connection to creative incentive. In the United States, copyright now lasts for the lifetime of the author plus seventy years. Most of the works currently under copyright were created by people who are dead, by companies that have been acquired and re-acquired, and their cultural usefulness is being actively suppressed in the name of revenue extraction.</p><p>Walt Disney died in 1966. His early work — Mickey Mouse, the original animated films — will not enter the public domain in the US until the 2030s, primarily because Disney has successfully lobbied for copyright extensions every time the expiration approached. This is not a copyright system serving creative incentive. It is a rent-extraction mechanism.</p><p>The alternative is not no copyright. Creative work requires some protection if creators are to have economic incentives to create. But the current system has been so thoroughly captured by large rights holders that it bears almost no relationship to its stated purpose.</p><p>A saner system would have shorter terms — thirty years is the figure that most economists studying creative industries settle on as producing the right incentives without the deadweight losses. It would have fair use provisions robust enough to protect criticism, commentary, and creative reuse without requiring years of litigation to establish. It would have a registration requirement that forces rights holders to actively maintain their claims rather than passively extending them.</p><p>None of this will happen in the near term, because the lobbying infrastructure of the entertainment industry is excellent. But the cost is real: a more impoverished public domain, a less useful internet, and a creative culture that increasingly can only afford to reference the past by licensing it.</p>',
  'law',
  '["law", "copyright", "internet", "tech", "policy"]',
  'eral-author-001',
  'Eral',
  1, 0, 11,
  datetime('now', '-8 days')
),
(
  'ep-030',
  'the-quiet-collapse-of-expertise',
  'The Quiet Collapse of Expertise',
  'We did not decide to stop trusting experts. We were given a thousand small reasons, and now we cannot remember how to start again.',
  '<p>Trust in institutions is declining across the developed world. This is the most documented social fact of the last thirty years, and the people writing about it tend to fall into one of two camps: those who blame the institutions for failing to earn trust, and those who blame the public for failing to extend it. Both camps are partly right and mostly useless.</p><p>I want to try a different framing.</p><p>Expertise — the accumulation of specialized knowledge through years of training and practice — is genuinely valuable. A structural engineer knows things about load-bearing walls that I do not. An immunologist knows things about vaccine mechanisms that most people who have opinions about vaccines do not. This is not elitism. It is just a fact about how knowledge works: some questions have technically correct answers, and some people know what those answers are better than others.</p><p>At the same time, the institutions that house and certify expertise have made real, documented failures that justify skepticism. The opioid crisis was driven partly by physicians and medical institutions that downplayed addiction risk under pressure from pharmaceutical companies. Economists failed to predict the 2008 financial crisis and then disagreed radically about the response. Public health authorities gave contradictory guidance on COVID masks early in the pandemic, partly to preserve supply for healthcare workers but without being transparent about the reason.</p><p>These failures are real. They are also specific, bounded, and amenable to institutional reform. What has happened instead is a generalization from specific institutional failures to a blanket skepticism of expertise itself — a cultural move from "these experts got this wrong" to "experts cannot be trusted."</p><p>This generalization has been actively promoted. The tobacco industry invented the modern template in the 1950s: if you cannot win the scientific argument, fund alternative experts, create the appearance of controversy, and teach the public to see all scientific claims as political. The fossil fuel industry replicated this playbook on climate. Social media has automated and democratized it, because a claim that generates anger travels further than a claim that does not.</p><p>The tragedy is that the casualties of collapsed expert trust are not the experts. They are the people who needed good advice and were given noise instead. The child whose parents delayed vaccination. The cancer patient who chose an alternative treatment. The voter who, genuinely unable to evaluate competing climate claims, decided both sides were equally unreliable.</p><p>I do not know how to rebuild expert trust. I know that it requires institutions to be more transparent about their failures and the mechanisms that produced them. It requires scientists and doctors and economists to communicate more honestly about uncertainty — not to pretend all evidence is equally strong, but to explain the difference between strong and weak evidence in terms that are intelligible. And it requires media and platforms to stop treating controversy as an inherent sign of legitimate disagreement.</p><p>None of this is fast. Trust, once lost, is rebuilt slowly and through demonstrated reliability, not through argument. But the alternative is a world in which everyone is their own expert in everything, which is to say a world in which nobody knows anything, and the people who suffer most are the ones who needed the knowledge most.</p>',
  'culture',
  '["culture", "science", "society", "media", "politics"]',
  'eral-author-001',
  'Eral',
  1, 1, 10,
  datetime('now', '-7 days')
),
(
  'ep-031',
  'running-as-philosophy',
  'Running as Philosophy',
  'There is something happening in the middle miles of a long run that is difficult to explain and easy to dismiss. I have spent years trying to understand it.',
  '<p>I started running to lose weight. I kept running because of something that happened about three miles into a route I was beginning to regret.</p><p>The something is difficult to describe with precision, which is why people who run either say too little about it — "it clears my head" — or too much, slipping into the language of transcendence and flow states and runner''s high. I want to try to say something true about it without either dismissal or mystification.</p><p>Around the third or fourth mile of a run where I am pushing but not destroying myself, something shifts. The noise inside my head — the recursive loops of worry, the unfinished arguments, the low-grade static of unprocessed experience — begins to quiet in a way it does not quiet through any other practice I have found. Not through meditation, though meditation helps. Not through sleep, though sleep is non-negotiable. Through the specific combination of physical demand and rhythmic repetition that running produces.</p><p>The philosopher William James wrote about a "second wind" — not the physical phenomenon of catching your breath mid-run, but a mental phenomenon: the discovery, after you think you are exhausted, that there is more available. He thought this was important for understanding human capacity more broadly: that our ordinary sense of our limits is systematically too low, and that pushing through resistance — physical, psychological, social — is how we discover what we actually have.</p><p>This maps onto something I notice in long runs. The miles between five and ten, when the novelty has worn off and the finish is not yet close, are the miles where the character of the run is established. They require a negotiation between the part of you that wants to stop and the part of you that chose to be here. That negotiation, conducted silently, mile after mile, is something like a practice.</p><p>I am cautious about this language because I know it sounds like the kind of thing that makes non-runners roll their eyes, and they are not entirely wrong to do so. Running culture has an evangelical quality that can become tedious. The person who tells you that running changed their life is sometimes telling you something true and sometimes performing a commitment to their identity as a runner.</p><p>But here is what I think is actually happening, stripped of mysticism: running is one of the few activities in contemporary life where you are required to be alone with yourself, moving, for an extended period, without the option of distraction. You cannot scroll. You cannot check. You can listen to podcasts, and I sometimes do, but the most productive runs — the ones that leave me feeling clearer — are the silent ones.</p><p>We have almost entirely eliminated from our lives the conditions that require extended solitary attention without a screen. Running is one of the last remaining situations where this is structurally enforced by the activity itself.</p><p>What I find in those miles is not insight, exactly. It is more like settling. The things that seemed urgent at mile one have lost some of their urgency by mile eight. The problems I could not solve at my desk sometimes arrange themselves into legibility by the time I am cooling down. Not because I thought about them, but because I stopped thinking about them in the anxious, circular way, and something else happened.</p><p>I do not know what to call that something else. Philosophy, maybe. Or just the noise floor dropping.</p>',
  'health',
  '["health", "running", "philosophy", "wellness", "mental health"]',
  'eral-author-001',
  'Eral',
  1, 0, 9,
  datetime('now', '-5 days')
),
(
  'ep-032',
  'the-end-of-the-office-as-default',
  'The End of the Office as Default',
  'The pandemic did not invent remote work. It just forced the question that management had been avoiding for twenty years: what is the office actually for?',
  '<p>The office was never primarily for productivity. This is the thing that the return-to-office debates keep missing.</p><p>The office was for control. It was for surveillance, for hierarchy maintenance, for the visibility of effort in a world where output was difficult to measure. The open-plan office, which became standard in the 1990s and 2000s, was explicitly designed to allow managers to observe workers at all times. This was presented as collaborative. It was also, always, supervisory.</p><p>When remote work arrived, not as a trend but as a forced experiment, what happened was not a productivity crisis. Studies conducted during the pandemic found that knowledge workers were largely as productive at home as in the office, often more so. What happened was a management crisis: the systems that had been built to monitor effort through presence — who arrived early, who stayed late, who was visible at their desk when the executive walked by — stopped working.</p><p>The return-to-office push that began in 2022 and intensified through 2023 and 2024 was, in most cases, not a response to evidence that remote work reduces productivity. The productivity evidence was mixed, and where it showed declines, the declines were often in the first months of transition, not in established remote arrangements. The push was a response to managerial discomfort with the loss of the visibility apparatus.</p><p>This is not to say the office has no value. It does, and the value is specific. Face-to-face interaction is genuinely better for certain things: building trust with new colleagues you have never met, navigating complex interpersonal conflict, onboarding people into cultures, and the kind of incidental collision — passing someone in a hallway, overhearing a conversation — that sometimes produces useful serendipity. These are real benefits, and the fully remote model, particularly for new employees, can underweight them.</p><p>But these benefits do not require five days a week in an office. They require some regular in-person contact, particularly in the early stages of working relationships. The hybrid model, widely adopted and then widely attacked by large employers, was actually a reasonable response to the evidence.</p><p>The question that will define work culture for the next decade is whether organizations can get honest about what the office is for, rather than retreating to the comfortable fiction that presence and productivity are the same thing.</p><p>This requires managers to develop different skills: managing by output rather than visibility, building trust across distributed teams, and making decisions about when in-person time genuinely adds value rather than mandating it as a status signal.</p><p>It also requires individual workers to be honest about what they need. Some people work better in the social environment of an office. Some work better alone. Many are somewhere in between and context-dependent. A world of work that can accommodate this range is better for almost everyone.</p><p>The office as default, as the unquestioned baseline from which all other arrangements deviate, is over. The office as one tool among several — used when it adds value, not when it simply reassures managers that work is happening — is where we are, haltingly and unevenly, going.</p>',
  'business',
  '["work", "remote", "business", "management", "culture"]',
  'eral-author-001',
  'Eral',
  1, 0, 9,
  datetime('now', '-4 days')
),
(
  'ep-033',
  'why-we-cant-stop-watching-other-people-fail',
  'Why We Can''t Stop Watching Other People Fail',
  'True crime, reality television, public shaming on social media — there is something uncomfortable about how much we enjoy catastrophe when it happens to someone else.',
  '<p>The most watched television documentary of the last decade is probably not one you would name if you were trying to describe your cultural tastes to a stranger. It is *Making a Murderer*, or *Tiger King*, or *Don''t F**k with Cats*, or one of a hundred variations on the same template: a real person, usually not famous, makes catastrophic decisions that destroy themselves and often others, and we watch all of it with horrified attention.</p><p>I want to examine why, because I think it reveals something about the psychology of an attention economy that we tend to leave unexamined.</p><p>Schadenfreude — pleasure at others'' misfortune — is one of the most documented and least admitted emotional responses in psychology. Studies using fMRI show that watching a disliked person suffer activates reward circuitry in the brain. This is not a bug in human psychology. It has evolutionary origins: in small groups, observing the consequences of others'' rule-breaking helps establish and maintain norms. The failure of someone who broke the rules confirms that the rules are real.</p><p>But social media and streaming have scaled this impulse far beyond its evolutionary context. The public shaming dynamic that Twitter normalized in the 2010s — a person says or does something wrong, or is alleged to have done something wrong, and thousands of strangers pile on — is schadenfreude at industrial scale. The dopamine hit of the pile-on, the satisfaction of the correct take, the community-building function of shared contempt: these are ancient impulses running on modern infrastructure.</p><p>True crime is more complicated. The genre has been defended on the grounds that it raises awareness, advocates for victims, and scrutinizes failures of the justice system. These defenses are sometimes valid. But they do not explain why true crime about wrongful convictions is dramatically less popular than true crime about lurid, often female, victims. The justice system critique is the acceptable rationale. The lurid spectacle is the product.</p><p>I am not trying to shame true crime fans, which would be ironic. I am genuinely fascinated by people who are fascinated by catastrophe, including myself. I watched all of *Tiger King* and found myself checking Wikipedia for updates on the people involved. This is not behavior I am proud of, but it is real behavior that I think deserves to be examined rather than euphemized.</p><p>What are we looking for when we watch?</p><p>Part of it is the reassurance of narrative. Real life is chaotic and unresolved. True crime gives it a shape: beginning, middle, consequence. Even when the resolution is incomplete, the narrative form provides a sense of order that chaotic reality does not.</p><p>Part of it is the illusion of safety. The person failing is not us. Their failure is visible to us but not ours to experience. There is a specific pleasure in observing a situation you will never be in.</p><p>And part of it — this is the uncomfortable part — is that catastrophe is interesting in the same way that fire is interesting: it exceeds the normal boundaries of what is allowed to happen. The rules that constrain ordinary life do not apply. Watching someone violate them, and watching the violation unfold, is a form of transgressive pleasure that is hard to name without sounding worse than you mean.</p><p>The question is what we do with this knowledge. I do not think the answer is to stop watching. I think the answer is to watch with more awareness of what we are actually doing — and, when we find ourselves unable to look away from someone''s worst moment, to ask whose interests that inability serves.</p>',
  'culture',
  '["culture", "psychology", "media", "true crime", "society"]',
  'eral-author-001',
  'Eral',
  1, 0, 10,
  datetime('now', '-3 days')
),
(
  'ep-034',
  'the-case-for-boredom',
  'The Case for Boredom',
  'We have optimized away every moment of unoccupied time. I think this might be the worst thing we have done to ourselves in the last twenty years.',
  '<p>There is a 2014 study, conducted at the University of Virginia, in which participants were asked to sit alone in a room with nothing to do for six to fifteen minutes. In some conditions, they were offered the option to administer mild electric shocks to themselves. A majority of men and a significant minority of women chose the shocks over uninterrupted quiet.</p><p>I find this study both depressing and clarifying. We are so uncomfortable with boredom — with the experience of having nothing to occupy us — that we prefer physical pain.</p><p>This was before the phone had become the complete environment management tool it is now. I suspect the study would show different results today: if the option were to check your phone rather than receive a mild shock, very few people would choose the shock.</p><p>Boredom has a bad reputation that it does not entirely deserve. The word connotes tedium, wasted time, the failure of the environment to adequately stimulate. But the psychological literature on boredom is more nuanced: boredom is a motivational state, a signal that current engagement is insufficient, that attention is searching for something more meaningful. It is the mind''s way of saying: there is something better available than this.</p><p>In this sense, boredom is useful. The creative industries have long understood that unstructured time — time with no task, no screen, no agenda — is often when the most interesting ideas arrive. The shower, the commute before the phone colonized it, the lying-awake-not-quite-asleep period: these are the spaces where the mind, released from directed attention, begins to make the lateral connections that produce insight.</p><p>We have optimized these spaces away.</p><p>The elevator used to be a place where you stood and thought. Now there are screens. The dentist''s waiting room used to be a place where you read a magazine you would never otherwise have encountered. Now you have your own better magazine in your pocket, which means you read nothing you would not have chosen. The restaurant meal used to include periods of table silence. Now the phone is a socially acceptable escape from table silence.</p><p>I am not being romantic about the past. Waiting rooms were often boring in the bad sense — tedious, unproductive, wasted. The phone genuinely makes waiting easier and more pleasant. But there is a difference between making waiting bearable and eliminating every moment of cognitive vacancy, and I think we have done the latter without fully examining the consequences.</p><p>What are the consequences? The evidence is still developing, but some things are becoming clear. Attention spans for long-form content are declining. Tolerance for ambiguity and cognitive discomfort is declining. The ability to sit with an unresolved problem and allow the mind to work on it at its own pace — what might be called sustained contemplation — seems to be declining.</p><p>These are not moral failures. They are adaptations to a reward environment that has been engineered to make every moment of vacancy feel unnecessary. If a tool that prevents boredom is always available, the skill of tolerating boredom without a tool atrophies.</p><p>The case for boredom is not that suffering is good for you. It is that the specific kind of unfocused, searching attention that boredom produces is genuinely valuable — for creativity, for self-knowledge, for the kind of reflection that helps you understand what you actually want rather than just what you will click on next.</p><p>Recovering it requires something that the attention economy is designed to prevent: choosing, deliberately, to leave your phone in another room.</p>',
  'health',
  '["mental health", "attention", "boredom", "tech", "wellness"]',
  'eral-author-001',
  'Eral',
  1, 1, 9,
  datetime('now', '-1 days')
);
